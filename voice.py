# ç³»çµ±éœ€æ±‚åˆ†æèˆ‡è¨­è¨ˆï¼šæ™ºæ…§å‹èªéŸ³è½‰éŒ„å™¨
# åŠŸèƒ½ï¼š
# 1. çµ¦ä¸€å€‹å½±ç‰‡ï¼ˆä»»æ„èªè¨€ï¼‰
# 2. æŠ½å–éŸ³è¨Šï¼ˆè½‰ç‚º wav æ ¼å¼ï¼‰
# 3. è‡ªå‹•èªè¨€è¾¨è­˜ + èªéŸ³è½‰æ–‡å­—ï¼ˆWhisperï¼‰
# 4. è‡ªå‹•åˆ†è¾¨ä¸åŒè¬›è©±äººï¼ˆResemblyzer + KMeansï¼‰
# 5. ç”¢ç”Ÿå«æ™‚é–“æˆ³çš„é€å­—ç¨¿ + è©±è€…æ¨™è¨» + è¼¸å‡º JSON

import os
import tempfile
import json
import numpy as np
from moviepy.editor import VideoFileClip, AudioFileClip
from resemblyzer import VoiceEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import whisper
import librosa
import soundfile as sf

def extract_audio(input_path: str, output_path: str = None) -> str:
    if output_path is None:
        output_path = tempfile.mktemp(suffix=".wav")
    clip = VideoFileClip(input_path)
    clip.audio.write_audiofile(output_path, fps=16000, nbytes=2, codec="pcm_s16le")
    return output_path


def estimate_num_speakers(embeddings: np.ndarray, max_speakers: int = 6) -> int:
    best_score, best_k = -1, 2
    for k in range(2, min(max_speakers, len(embeddings))):
        kmeans = KMeans(n_clusters=k, random_state=0).fit(embeddings)
        score = silhouette_score(embeddings, kmeans.labels_)
        if score > best_score:
            best_score, best_k = score, k
    return best_k


def speaker_diarization(audio_path: str, window_size_sec: float = 1.5, num_speakers: int = None):
    wav, sr = librosa.load(audio_path, sr=16000)
    encoder = VoiceEncoder()
    slices, timestamps = [], []
    for i in range(0, len(wav), int(sr * window_size_sec)):
        chunk = wav[i:i + int(sr * window_size_sec)]
        if len(chunk) == int(sr * window_size_sec):
            slices.append(chunk)
            timestamps.append(i / sr)
    embeddings = np.array([encoder.embed_utterance(chunk) for chunk in slices])

    if num_speakers is None:
        num_speakers = estimate_num_speakers(embeddings)
        print(f"ğŸ” è‡ªå‹•æ¨ä¼°èªªè©±äººæ•¸ï¼š{num_speakers}")

    kmeans = KMeans(n_clusters=num_speakers, random_state=0).fit(embeddings)
    labels = kmeans.labels_
    segments = [
        {"start": round(timestamps[i], 2), "end": round(timestamps[i] + window_size_sec, 2), "speaker": f"Speaker {label}"}
        for i, label in enumerate(labels)
    ]
    return segments


def merge_transcript_with_speakers(transcript_segments, speaker_segments):
    result = []
    for seg in transcript_segments:
        ts_start = seg["start"]
        ts_end = seg["end"]
        matched = next((s for s in speaker_segments if s["start"] <= ts_start <= s["end"]), None)
        speaker = matched["speaker"] if matched else "Unknown"
        result.append({
            "start": ts_start,
            "end": ts_end,
            "speaker": speaker,
            "text": seg["text"].strip()
        })
    return result


def analyze_video(input_path: str, num_speakers: int = None, output_json: str = "transcript.json"):
    print("ğŸï¸ Extracting audio from video...")
    audio_path = extract_audio(input_path)

    print("ğŸ§  Loading Whisper and transcribing...")
    model = whisper.load_model("base")
    result = model.transcribe(audio_path, task="transcribe")  # è‡ªå‹•èªè¨€è¾¨è­˜

    print("ğŸ§ Running speaker diarization...")
    speaker_segments = speaker_diarization(audio_path, num_speakers=num_speakers)

    print("ğŸ“œ Merging transcription and speakers...")
    merged = merge_transcript_with_speakers(result["segments"], speaker_segments)

    for entry in merged:
        sm, ss = divmod(entry["start"], 60)
        em, es = divmod(entry["end"], 60)
        print(f"[{int(sm):02d}:{int(ss):02d}-{int(em):02d}:{int(es):02d}] {entry['speaker']}: {entry['text']}")

    print(f"ğŸ’¾ Saving result to {output_json}...")
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)

    return merged


# âœ… ä½¿ç”¨ç¯„ä¾‹ï¼š
if __name__ == "__main__":
    video_path = "sample_video.mp4"
    analyze_video(video_path, num_speakers=None, output_json="output.json")